{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f74d033b",
      "metadata": {},
      "source": [
        "## Notebook exemplificando o uso das funcionalidades do YOLO-MS\n",
        "Representação do fluxo de trabalho, desde o treinamento até a inferência."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "f4b9bf95",
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2\n",
        "import json\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def convert_to_coco(dataset: str, output_dir: str, train_ratio: float = 0.6, val_ratio: float = 0.2):\n",
        "    \"\"\"\n",
        "    Converte dataset pedrozamboni para formato COCO com divisão 60/20/20.\n",
        "    \"\"\"\n",
        "\n",
        "    dataset_dir = Path(dataset)\n",
        "    output_dir = Path(output_dir)\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Cria estrutura do formato COCO com pasta de teste\n",
        "    (output_dir / \"images\" / \"train\").mkdir(parents=True, exist_ok=True)\n",
        "    (output_dir / \"images\" / \"val\").mkdir(parents=True, exist_ok=True)\n",
        "    (output_dir / \"images\" / \"test\").mkdir(parents=True, exist_ok=True)\n",
        "    (output_dir / \"annotations\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Obtém arquivos de anotação\n",
        "    bbox_files = list((dataset_dir / \"bbox_txt\").glob(\"*.txt\"))\n",
        "    image_files = list((dataset_dir / \"images\").glob(\"*.png\"))\n",
        "\n",
        "    matched_data = []\n",
        "    for bbox_file in bbox_files:\n",
        "        img_id = bbox_file.stem\n",
        "        img_file = None\n",
        "\n",
        "        for img_file in image_files:\n",
        "            if img_file.stem == img_id:\n",
        "                img_file = img_file\n",
        "                break\n",
        "\n",
        "        if img_file and img_file.exists():\n",
        "            matched_data.append((img_file, bbox_file))\n",
        "\n",
        "    print(f\"Encontrados {len(matched_data)} arquivos de imagem e bbox correspondentes.\")\n",
        "    \n",
        "    # Primeira divisão: 60% treino, 40% temporário (para val + teste)\n",
        "    train_data, temp_data = train_test_split(\n",
        "        matched_data, \n",
        "        train_size=train_ratio, \n",
        "        random_state=42\n",
        "    )\n",
        "    \n",
        "    # Segunda divisão: 20% val, 20% teste dos 40% restantes\n",
        "    val_ratio_adjusted = val_ratio / (1 - train_ratio)  # 0.2 / 0.4 = 0.5\n",
        "    val_data, test_data = train_test_split(\n",
        "        temp_data, \n",
        "        train_size=val_ratio_adjusted, \n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    print(f\"Divisão dos dados:\")\n",
        "    print(f\"  Treino: {len(train_data)} imagens ({len(train_data)/len(matched_data)*100:.1f}%)\")\n",
        "    print(f\"  Val:    {len(val_data)} imagens ({len(val_data)/len(matched_data)*100:.1f}%)\")\n",
        "    print(f\"  Teste:  {len(test_data)} imagens ({len(test_data)/len(matched_data)*100:.1f}%)\")\n",
        "\n",
        "    # Processa datasets de treino, validação e teste\n",
        "    for split, data in [(\"train\", train_data), (\"val\", val_data), (\"test\", test_data)]:\n",
        "        images = []\n",
        "        annotations = []\n",
        "        annotation_id = 1\n",
        "\n",
        "        for img_id, (img_file, bbox_file) in enumerate(data):\n",
        "            # Lê imagem\n",
        "            img = cv2.imread(str(img_file))\n",
        "            if img is None:\n",
        "                print(f\"Aviso: Não foi possível ler a imagem {img_file}. Pulando.\")\n",
        "                continue\n",
        "\n",
        "            height, width = img.shape[:2]\n",
        "\n",
        "            # Copia imagem para diretório de saída\n",
        "            output_img_path = output_dir / \"images\" / split / img_file.name\n",
        "            cv2.imwrite(str(output_img_path), img)\n",
        "\n",
        "            images.append({\n",
        "                \"id\": img_id,\n",
        "                \"file_name\": img_file.name,\n",
        "                \"width\": width,\n",
        "                \"height\": height\n",
        "            })\n",
        "\n",
        "            # Lê bounding boxes\n",
        "            with open(bbox_file, \"r\") as f:\n",
        "                for line in f:\n",
        "                    line = line.strip()\n",
        "                    if not line:\n",
        "                        continue\n",
        "\n",
        "                    parts = line.split() \n",
        "                    if len(parts) == 4:\n",
        "                        x1, y1, x2, y2 = map(float, parts)\n",
        "\n",
        "                        # Converte para formato COCO (x, y, largura, altura)\n",
        "                        x = min(x1, x2)\n",
        "                        y = min(y1, y2)\n",
        "                        bbox_width = abs(x2 - x1)\n",
        "                        bbox_height = abs(y2 - y1)\n",
        "                        \n",
        "                        # Garante que as coordenadas estão dentro dos limites da imagem\n",
        "                        x = max(0, min(x, width - 1))\n",
        "                        y = max(0, min(y, height - 1))\n",
        "                        bbox_width = min(bbox_width, width - x)\n",
        "                        bbox_height = min(bbox_height, height - y)\n",
        "\n",
        "                        if bbox_width > 0 and bbox_height > 0:\n",
        "                            annotations.append({\n",
        "                                \"id\": annotation_id,\n",
        "                                \"image_id\": img_id,\n",
        "                                \"category_id\": 1,  # Assumindo uma única categoria\n",
        "                                \"bbox\": [x, y, bbox_width, bbox_height],\n",
        "                                \"area\": bbox_width * bbox_height,\n",
        "                                \"iscrowd\": 0\n",
        "                            })\n",
        "                            annotation_id += 1\n",
        "                    else:\n",
        "                        print(f\"Aviso: Formato de bbox inesperado em {bbox_file}. Esperado 4 valores, obtido {len(parts)}. Pulando linha: {line}\")\n",
        "        \n",
        "        # Cria formato COCO\n",
        "        coco_format = {\n",
        "            \"images\": images,\n",
        "            \"annotations\": annotations,\n",
        "            \"categories\": [\n",
        "                {\n",
        "                    \"id\": 1,\n",
        "                    \"name\": \"tree\",\n",
        "                    \"supercategory\": \"plant\"\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "        \n",
        "        # Salva arquivo de anotação\n",
        "        with open(output_dir / \"annotations\" / f\"instances_{split}.json\", 'w') as f:\n",
        "            json.dump(coco_format, f, indent=2)\n",
        "        \n",
        "        print(f\"✅ {split}: {len(images)} imagens, {len(annotations)} anotações\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "50c0c17b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encontrados 220 arquivos de imagem e bbox correspondentes.\n",
            "Divisão dos dados:\n",
            "  Treino: 132 imagens (60.0%)\n",
            "  Val:    44 imagens (20.0%)\n",
            "  Teste:  44 imagens (20.0%)\n",
            "✅ train: 132 imagens, 2014 anotações\n",
            "✅ train: 132 imagens, 2014 anotações\n",
            "✅ val: 44 imagens, 707 anotações\n",
            "✅ val: 44 imagens, 707 anotações\n",
            "✅ test: 44 imagens, 632 anotações\n",
            "✅ test: 44 imagens, 632 anotações\n"
          ]
        }
      ],
      "source": [
        "convert_to_coco(\n",
        "    dataset=\"pedrozamboni_dataset\",\n",
        "    output_dir=\"coco_dataset\",\n",
        "    train_ratio=0.6,  # 60% train\n",
        "    val_ratio=0.2     # 20% val\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbf4e8ee",
      "metadata": {},
      "source": [
        "## Criação de um config file para o YOLO-MS Fine-tune\n",
        "Ajuste de hiperparâmetros e configuração do modelo YOLO-MS para o treinamento. configuração apenas de uma única classe ('tree') para o fine-tune do modelo YOLO-MS de 23M de parâmetros."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "40243fbf",
      "metadata": {},
      "outputs": [],
      "source": [
        "#_base_ = 'yoloms_syncbn_fast_8xb32-300e_coco.py'\n",
        "_base_ = 'mmyolo/configs/yoloms/yoloms_syncbn_fast_8xb32-300e_coco.py'\n",
        "# Configuração do dataset\n",
        "data_root = 'D:/UnB/IIA/YOLO-MS-IIA/coco_dataset'\n",
        "class_name = ('tree',)  # única classe\n",
        "num_classes = 1\n",
        "metainfo = dict(classes=class_name, palette=[(0, 255, 0)])\n",
        "\n",
        "# Parâmetros de treinamento para fine-tuning\n",
        "max_epochs = 300  # Reduzido de 300 para fine-tuning\n",
        "train_batch_size_per_gpu = 2\n",
        "val_batch_size_per_gpu = 1\n",
        "train_num_workers = 2\n",
        "val_num_workers = 1\n",
        "\n",
        "# Taxa de aprendizado menor para fine-tuning\n",
        "base_lr = 0.0001\n",
        "\n",
        "\n",
        "# Configuração do modelo - Atualizar para classe única\n",
        "model = dict(\n",
        "    bbox_head=dict(\n",
        "        head_module=dict(num_classes=num_classes)\n",
        "    ),\n",
        "    train_cfg=dict(\n",
        "        assigner=dict(num_classes=num_classes)\n",
        "    )\n",
        ")\n",
        "\n",
        "# Configuração dos dados\n",
        "train_dataloader = dict(\n",
        "    batch_size=train_batch_size_per_gpu,\n",
        "    num_workers=train_num_workers,\n",
        "    dataset=dict(\n",
        "        data_root=data_root,\n",
        "        ann_file='annotations/instances_train.json',\n",
        "        data_prefix=dict(img='images/train/'),\n",
        "        metainfo=metainfo\n",
        "    )\n",
        ")\n",
        "\n",
        "val_dataloader = dict(\n",
        "    batch_size=val_batch_size_per_gpu,\n",
        "    num_workers=val_num_workers,\n",
        "    dataset=dict(\n",
        "        data_root=data_root,\n",
        "        ann_file='annotations/instances_val.json',\n",
        "        data_prefix=dict(img='images/val/'),\n",
        "        metainfo=metainfo,\n",
        "        test_mode=True\n",
        "    )\n",
        ")\n",
        "\n",
        "# Adicionar dataloader de teste\n",
        "test_dataloader = dict(\n",
        "    batch_size=val_batch_size_per_gpu,\n",
        "    num_workers=val_num_workers,\n",
        "    dataset=dict(\n",
        "        data_root=data_root,\n",
        "        ann_file='annotations/instances_test.json',\n",
        "        data_prefix=dict(img='images/test/'),\n",
        "        metainfo=metainfo,\n",
        "        test_mode=True\n",
        "    )\n",
        ")\n",
        "\n",
        "# Avaliadores\n",
        "val_evaluator = dict(\n",
        "    type='mmdet.CocoMetric',\n",
        "    ann_file=f'{data_root}/annotations/instances_val.json',\n",
        "    metric='bbox',\n",
        "    format_only=False,\n",
        "    classwise=True,\n",
        ")\n",
        "\n",
        "test_evaluator = dict(\n",
        "    type='mmdet.CocoMetric',\n",
        "    ann_file=f'{data_root}/annotations/instances_test.json',\n",
        "    metric='bbox',\n",
        "    format_only=False,\n",
        "    classwise=True,\n",
        ")\n",
        "\n",
        "# Configuração de treinamento\n",
        "train_cfg = dict(\n",
        "    type='EpochBasedTrainLoop',\n",
        "    max_epochs=max_epochs,\n",
        "    val_interval=20\n",
        ")\n",
        "\n",
        "val_cfg = dict(type='ValLoop')\n",
        "test_cfg = dict(type='TestLoop')\n",
        "    \n",
        "# Escalonamento automático da taxa de aprendizado\n",
        "auto_scale_lr = dict(enable=True, base_batch_size=16)\n",
        "\n",
        "# Configuração de hooks\n",
        "default_hooks = dict(\n",
        "    checkpoint=dict(\n",
        "        type='CheckpointHook',\n",
        "        interval=20,\n",
        "        save_best='coco/bbox_mAP',\n",
        "        rule='greater',\n",
        "        max_keep_ckpts=5\n",
        "    ),\n",
        "    logger=dict(type='LoggerHook', interval=20)\n",
        ")\n",
        "\n",
        "# Carregar pesos pré-treinados para fine-tuning\n",
        "load_from = 'D:/UnB/IIA/YOLO-MS-IIA/pre-trained.pth'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6c6d546",
      "metadata": {},
      "source": [
        "Esse código está salvo no arquivo `yoloms_trees_finetune.py` e pode ser utilizado para treinar o modelo YOLO-MS com os dados do dataset convertido para o formato COCO."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea32dcdb",
      "metadata": {},
      "source": [
        "## Fine-tune do modelo\n",
        "Como o dataset COCO e o arquivo de configuração do modelo, o próximo passo é realizar o treinamento de pesos do modelo YOLO-MS com os dados do dataset convertido."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df77f235",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------\n",
            "System environment:\n",
            "    sys.platform: win32\n",
            "    Python: 3.8.20 (default, Oct  3 2024, 15:19:54) [MSC v.1929 64 bit (AMD64)]\n",
            "    CUDA available: True\n",
            "    numpy_random_seed: 633647791\n",
            "    GPU 0: NVIDIA GeForce RTX 2060 SUPER\n",
            "    CUDA_HOME: C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.6\n",
            "    NVCC: Cuda compilation tools, release 12.6, V12.6.20\n",
            "    GCC: n/a\n",
            "    PyTorch: 1.12.1\n",
            "    PyTorch compiling details: PyTorch built with:\n",
            "  - C++ Version: 199711\n",
            "  - MSVC 192829337\n",
            "  - Intel(R) Math Kernel Library Version 2020.0.2 Product Build 20200624 for Intel(R) 64 architecture applications\n",
            "  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)\n",
            "  - OpenMP 2019\n",
            "  - LAPACK is enabled (usually provided by MKL)\n",
            "  - CPU capability usage: AVX2\n",
            "  - CUDA Runtime 11.3\n",
            "  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37\n",
            "  - CuDNN 8.3.2  (built against CUDA 11.5)\n",
            "  - Magma 2.5.4\n",
            "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.3.2, CXX_COMPILER=C:/cb/pytorch_1000000000000/work/tmp_bin/sccache-cl.exe, CXX_FLAGS=/DWIN32 /D_WINDOWS /GR /EHsc /w /bigobj -DUSE_PTHREADPOOL -openmp:experimental -IC:/cb/pytorch_1000000000000/work/mkl/include -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOCUPTI -DUSE_FBGEMM -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.12.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=OFF, USE_NNPACK=OFF, USE_OPENMP=ON, USE_ROCM=OFF, \n",
            "\n",
            "    TorchVision: 0.13.1\n",
            "    OpenCV: 4.11.0\n",
            "    MMEngine: 0.7.1\n",
            "\n",
            "Runtime environment:\n",
            "    cudnn_benchmark: True\n",
            "    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}\n",
            "    dist_cfg: {'backend': 'nccl'}\n",
            "    seed: None\n",
            "    Distributed launcher: none\n",
            "    Distributed training: False\n",
            "    GPU number: 1\n",
            "------------------------------------------------------------\n",
            "07/01 20:04:43 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - bbox_mAP_copypaste: 0.382 0.706 0.389 0.222 0.436 0.598\n",
            "07/01 20:04:43 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [20][44/44]  coco/tree_precision: 0.3820  coco/bbox_mAP: 0.3820  coco/bbox_mAP_50: 0.7060  coco/bbox_mAP_75: 0.3890  coco/bbox_mAP_s: 0.2220  coco/bbox_mAP_m: 0.4360  coco/bbox_mAP_l: 0.5980  data_time: 0.0795  time: 0.1512\n",
            "07/01 20:04:46 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - The best checkpoint with 0.3820 coco/bbox_mAP at 20 epoch is saved to best_coco/bbox_mAP_epoch_20.pth.\n",
            "07/01 20:04:52 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [21][20/66]  lr: 5.0000e-04  eta: 1:38:07  time: 0.3205  data_time: 0.0027  memory: 2861  loss: 0.9321  loss_cls: 0.4522  loss_bbox: 0.4799\n",
            "07/01 20:04:59 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [21][40/66]  lr: 5.0000e-04  eta: 1:37:57  time: 0.3131  data_time: 0.0026  memory: 2869  loss: 0.9301  loss_cls: 0.4515  loss_bbox: 0.4786\n",
            "07/01 20:05:05 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [21][60/66]  lr: 5.0000e-04  eta: 1:37:51  time: 0.3122  data_time: 0.0006  memory: 2862  loss: 0.9336  loss_cls: 0.4487  loss_bbox: 0.4849\n",
            ".\n",
            ".\n",
            ".\n",
            "(Output do treinamento truncado para brevidade)\n",
            ".\n",
            ".\n",
            ".\n",
            "07/01 20:45:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: yoloms_trees_finetune_20250701_195721\n",
            "07/01 20:47:23 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [160][20/66]  lr: 4.9717e-04  eta: 0:43:00  time: 0.3241  data_time: 0.0025  memory: 2851  loss: 0.7034  loss_cls: 0.2879  loss_bbox: 0.4156\n",
            "07/01 20:47:29 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [160][40/66]  lr: 4.9698e-04  eta: 0:42:55  time: 0.3236  data_time: 0.0024  memory: 2837  loss: 0.6982  loss_cls: 0.2852  loss_bbox: 0.4130\n",
            "07/01 20:47:36 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [160][60/66]  lr: 4.9679e-04  eta: 0:42:50  time: 0.3248  data_time: 0.0010  memory: 2841  loss: 0.6776  loss_cls: 0.2728  loss_bbox: 0.4048\n",
            "07/01 20:47:38 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: yoloms_trees_finetune_20250701_195721\n",
            "07/01 20:47:38 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 160 epochs\n",
            "07/01 20:47:43 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [160][20/44]    eta: 0:00:01  time: 0.0402  data_time: 0.0005  memory: 2847  \n",
            "07/01 20:47:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [160][40/44]    eta: 0:00:00  time: 0.0427  data_time: 0.0005  memory: 556  \n",
            "07/01 20:47:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Evaluating bbox...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.86s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.03s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.417\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.751\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.425\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.241\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.474\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.646\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.048\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.338\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.547\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.390\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.602\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.739\n",
            "07/01 20:47:45 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
            "+----------+-------+--------+--------+-------+-------+-------+\n",
            "| category | mAP   | mAP_50 | mAP_75 | mAP_s | mAP_m | mAP_l |\n",
            "+----------+-------+--------+--------+-------+-------+-------+\n",
            "| tree     | 0.417 | 0.751  | 0.425  | 0.241 | 0.474 | 0.646 |\n",
            "+----------+-------+--------+--------+-------+-------+-------+\n",
            ".\n",
            ".\n",
            ".\n",
            "(Output do treinamento truncado para brevidade)\n",
            ".\n",
            ".\n",
            "."
          ]
        }
      ],
      "source": [
        "!python mmyolo/tools/train.py \"D:/UnB/IIA/YOLO-MS-IIA/yoloms_trees_finetune.py\" --work-dir \"D:/UnB/IIA/YOLO-MS-IIA/finetune\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9982381a",
      "metadata": {},
      "source": [
        "## Teste do modelo treinado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "30923478",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loads checkpoint by local backend from path: finetune/best_coco/best_epoch_200.pth\n",
            "07/01 21:06:57 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - `Visualizer` backend is not initialized because save_dir is None.\n",
            "[                                                  ] 0/1, elapsed: 0s, ETA:07/01 21:06:57 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"FileClient\" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io\n",
            "07/01 21:06:57 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"HardDiskBackend\" is the alias of \"LocalBackend\" and the former will be deprecated in future.\n",
            "\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 1/1, 0.3 task/s, elapsed: 3s, ETA:     0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "e:\\Anaconda\\envs\\yolo\\lib\\site-packages\\albumentations\\__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.8 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n",
            "e:\\Anaconda\\envs\\yolo\\lib\\site-packages\\torch\\functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\TensorShape.cpp:2895.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "e:\\Anaconda\\envs\\yolo\\lib\\site-packages\\mmengine\\visualization\\visualizer.py:741: UserWarning: Warning: The bbox is out of bounds, the drawn bbox may not be in the image\n",
            "  warnings.warn(\n",
            "e:\\Anaconda\\envs\\yolo\\lib\\site-packages\\mmengine\\visualization\\visualizer.py:812: UserWarning: Warning: The polygon is out of bounds, the drawn polygon may not be in the image\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "!python mmyolo/demo/image_demo.py \"finetune/one-stage.png\" \"yoloms_trees_finetune.py\" \"finetune/best_coco/best_epoch_200.pth\" --show"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a8619f0",
      "metadata": {},
      "source": [
        "![one-stage-yoloms](finetune/one-stage-yoloms.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "807f70f5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loads checkpoint by local backend from path: finetune/best_coco/best_epoch_200.pth\n",
            "07/01 21:07:06 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - `Visualizer` backend is not initialized because save_dir is None.\n",
            "[                                                  ] 0/1, elapsed: 0s, ETA:07/01 21:07:06 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"FileClient\" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io\n",
            "07/01 21:07:06 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"HardDiskBackend\" is the alias of \"LocalBackend\" and the former will be deprecated in future.\n",
            "\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 1/1, 0.4 task/s, elapsed: 3s, ETA:     0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "e:\\Anaconda\\envs\\yolo\\lib\\site-packages\\albumentations\\__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.8 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n",
            "e:\\Anaconda\\envs\\yolo\\lib\\site-packages\\torch\\functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\TensorShape.cpp:2895.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "e:\\Anaconda\\envs\\yolo\\lib\\site-packages\\mmengine\\visualization\\visualizer.py:741: UserWarning: Warning: The bbox is out of bounds, the drawn bbox may not be in the image\n",
            "  warnings.warn(\n",
            "e:\\Anaconda\\envs\\yolo\\lib\\site-packages\\mmengine\\visualization\\visualizer.py:812: UserWarning: Warning: The polygon is out of bounds, the drawn polygon may not be in the image\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "!python mmyolo/demo/image_demo.py \"finetune/two-stage.png\" \"yoloms_trees_finetune.py\" \"finetune/best_coco/best_epoch_200.pth\" --show"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8b0b314",
      "metadata": {},
      "source": [
        "![two-stage-yoloms](finetune/two-stage-yoloms.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5c58444",
      "metadata": {},
      "source": [
        "## Gráfico de perda até a epoca 25\n",
        "![loss-epoch-25](grafico.png)\n",
        "\n",
        "## Tabela comparativa entre os resultados dos diferentes modelos testados no experimento\n",
        "\n",
        "# Resultados\n",
        "\n",
        "| Model                                              | Test Set AP50 |\n",
        "| :------------------------------------------------- | :-----------: |\n",
        "| **Anchor-Free (AF) Methods**                       |               |\n",
        "| FSAF                                               |     0.701     |\n",
        "| ATSS                                               |     0.692     |\n",
        "| FoveaBox                                           |     0.692     |\n",
        "| VarifocalNet (2)                                   |     0.683     |\n",
        "| VarifocalNet (1)                                   |     0.664     |\n",
        "| **One-Stage Methods**                              |               |\n",
        "| YOLO-MS (Epoch 200)                                |     0.748     |\n",
        "| YOLO-MS (Epoch 300)                                |     0.739     |\n",
        "| Gradient Harmonized Single-stage Detector          |     0.691     |\n",
        "| Generalized Focal Loss                             |     0.677     |\n",
        "| Probabilistic Anchor Assignment                    |     0.677     |\n",
        "| SABL                                               |     0.661     |\n",
        "| NAS-FPN                                            |     0.658     |\n",
        "| RetinaNet                                          |     0.650     |\n",
        "| YoloV3                                             |     0.591     |\n",
        "| **Two-Stage and Multi-Stage (DetectorRS) Methods** |               |\n",
        "| Double Heads                                       |     0.699     |\n",
        "| CARAFE                                             |     0.697     |\n",
        "| Empirical Attention                                |     0.690     |\n",
        "| Mixed precision training                           |     0.679     |\n",
        "| Faster R-CNN                                       |     0.660     |\n",
        "| Deformable ConvNets v2                             |     0.657     |\n",
        "| Dynamic R-CNN                                      |     0.655     |\n",
        "| DetecorRS                                          |     0.651     |\n",
        "| Weight Standardization                             |     0.631     |\n",
        "\n",
        "# Comparação dos resultados do modelo YOLO-MS treinado com diferentes épocas\n",
        "\n",
        "| model     |  mAP  | mAP_50 | mAP_75 | mAP_s | mAP_m | mAP_l |\n",
        "| :-------- | :---: | :----: | :----: | :---: | :---: | :---: |\n",
        "| Epoch 300 | 0.405 | 0.739  | 0.403  | 0.227 | 0.449 | 0.653 |\n",
        "| Epoch 200 | 0.416 | 0.748  | 0.421  | 0.246 | 0.463 | 0.616 |\n",
        "\n",
        "Diferenças utilizadas nesse projeto\n",
        "\n",
        "- arquivo de config para fine tune\n",
        "- script para conversão para dataset coco"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "872d6e89",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "yolo",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
