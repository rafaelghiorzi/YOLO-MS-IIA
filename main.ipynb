{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f74d033b",
   "metadata": {},
   "source": [
    "## Notebook exemplificando o uso das funcionalidades do YOLO-MS\n",
    "Representação do fluxo de trabalho, desde o treinamento até a inferência."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4b9bf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def convert_to_coco(dataset: str, output_dir: str, train_ratio: float = 0.6, val_ratio: float = 0.2):\n",
    "    \"\"\"\n",
    "    Convert pedrozamboni dataset to COCO format with 60/20/20 split.\n",
    "    \"\"\"\n",
    "\n",
    "    dataset_dir = Path(dataset)\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Create COCO format structure with test folder\n",
    "    (output_dir / \"images\" / \"train\").mkdir(parents=True, exist_ok=True)\n",
    "    (output_dir / \"images\" / \"val\").mkdir(parents=True, exist_ok=True)\n",
    "    (output_dir / \"images\" / \"test\").mkdir(parents=True, exist_ok=True)  # Added test folder\n",
    "    (output_dir / \"annotations\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Get annotation files\n",
    "    bbox_files = list((dataset_dir / \"bbox_txt\").glob(\"*.txt\"))\n",
    "    image_files = list((dataset_dir / \"images\").glob(\"*.png\"))\n",
    "\n",
    "    matched_data = []\n",
    "    for bbox_file in bbox_files:\n",
    "        img_id = bbox_file.stem\n",
    "        img_file = None\n",
    "\n",
    "        for img_file in image_files:\n",
    "            if img_file.stem == img_id:\n",
    "                img_file = img_file\n",
    "                break\n",
    "\n",
    "        if img_file and img_file.exists():\n",
    "            matched_data.append((img_file, bbox_file))\n",
    "\n",
    "    print(f\"Found {len(matched_data)} matching image and bbox files.\")\n",
    "    \n",
    "    # First split: 60% train, 40% temp (for val + test)\n",
    "    train_data, temp_data = train_test_split(\n",
    "        matched_data, \n",
    "        train_size=train_ratio, \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Second split: 20% val, 20% test from the remaining 40%\n",
    "    val_ratio_adjusted = val_ratio / (1 - train_ratio)  # 0.2 / 0.4 = 0.5\n",
    "    val_data, test_data = train_test_split(\n",
    "        temp_data, \n",
    "        train_size=val_ratio_adjusted, \n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    print(f\"Data split:\")\n",
    "    print(f\"  Train: {len(train_data)} images ({len(train_data)/len(matched_data)*100:.1f}%)\")\n",
    "    print(f\"  Val:   {len(val_data)} images ({len(val_data)/len(matched_data)*100:.1f}%)\")\n",
    "    print(f\"  Test:  {len(test_data)} images ({len(test_data)/len(matched_data)*100:.1f}%)\")\n",
    "\n",
    "    # Process train, val, and test datasets\n",
    "    for split, data in [(\"train\", train_data), (\"val\", val_data), (\"test\", test_data)]:\n",
    "        images = []\n",
    "        annotations = []\n",
    "        annotation_id = 1\n",
    "\n",
    "        for img_id, (img_file, bbox_file) in enumerate(data):\n",
    "            # Read image\n",
    "            img = cv2.imread(str(img_file))\n",
    "            if img is None:\n",
    "                print(f\"Warning: Could not read image {img_file}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            height, width = img.shape[:2]\n",
    "\n",
    "            # Copy image to output directory\n",
    "            output_img_path = output_dir / \"images\" / split / img_file.name\n",
    "            cv2.imwrite(str(output_img_path), img)\n",
    "\n",
    "            images.append({\n",
    "                \"id\": img_id,\n",
    "                \"file_name\": img_file.name,\n",
    "                \"width\": width,\n",
    "                \"height\": height\n",
    "            })\n",
    "\n",
    "            # Read bounding boxes\n",
    "            with open(bbox_file, \"r\") as f:\n",
    "                for line in f:\n",
    "                    line = line.strip()\n",
    "                    if not line:\n",
    "                        continue\n",
    "\n",
    "                    parts = line.split() \n",
    "                    if len(parts) == 4:\n",
    "                        x1, y1, x2, y2 = map(float, parts)\n",
    "\n",
    "                        # Convert to COCO format (x, y, width, height)\n",
    "                        x = min(x1, x2)\n",
    "                        y = min(y1, y2)\n",
    "                        bbox_width = abs(x2 - x1)\n",
    "                        bbox_height = abs(y2 - y1)\n",
    "                        \n",
    "                        # Ensure coordinates are within image bounds\n",
    "                        x = max(0, min(x, width - 1))\n",
    "                        y = max(0, min(y, height - 1))\n",
    "                        bbox_width = min(bbox_width, width - x)\n",
    "                        bbox_height = min(bbox_height, height - y)\n",
    "\n",
    "                        if bbox_width > 0 and bbox_height > 0:\n",
    "                            annotations.append({\n",
    "                                \"id\": annotation_id,\n",
    "                                \"image_id\": img_id,\n",
    "                                \"category_id\": 1,  # Assuming a single category\n",
    "                                \"bbox\": [x, y, bbox_width, bbox_height],\n",
    "                                \"area\": bbox_width * bbox_height,\n",
    "                                \"iscrowd\": 0\n",
    "                            })\n",
    "                            annotation_id += 1\n",
    "                    else:\n",
    "                        print(f\"Warning: Unexpected bbox format in {bbox_file}. Expected 4 values, got {len(parts)}. Skipping line: {line}\")\n",
    "        \n",
    "        # Create COCO format\n",
    "        coco_format = {\n",
    "            \"images\": images,\n",
    "            \"annotations\": annotations,\n",
    "            \"categories\": [\n",
    "                {\n",
    "                    \"id\": 1,\n",
    "                    \"name\": \"tree\",\n",
    "                    \"supercategory\": \"plant\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Save annotation file\n",
    "        with open(output_dir / \"annotations\" / f\"instances_{split}.json\", 'w') as f:\n",
    "            json.dump(coco_format, f, indent=2)\n",
    "        \n",
    "        print(f\"✅ {split}: {len(images)} images, {len(annotations)} annotations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50c0c17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 220 matching image and bbox files.\n",
      "Data split:\n",
      "  Train: 132 images (60.0%)\n",
      "  Val:   44 images (20.0%)\n",
      "  Test:  44 images (20.0%)\n",
      "✅ train: 132 images, 2014 annotations\n",
      "✅ val: 44 images, 707 annotations\n",
      "✅ test: 44 images, 632 annotations\n"
     ]
    }
   ],
   "source": [
    "convert_to_coco(\n",
    "    dataset=\"pedrozamboni_dataset\",\n",
    "    output_dir=\"coco_dataset\",\n",
    "    train_ratio=0.6,  # 60% train\n",
    "    val_ratio=0.2     # 20% val\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf4e8ee",
   "metadata": {},
   "source": [
    "## Criação de um config file para o YOLO-MS Fine-tune\n",
    "Ajuste de hiperparâmetros e configuração do modelo YOLO-MS para o treinamento. configuração apenas de uma única classe ('tree') para o fine-tune do modelo YOLO-MS de 23M de parâmetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40243fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#_base_ = 'yoloms_syncbn_fast_8xb32-300e_coco.py'\n",
    "_base_ = 'mmyolo/configs/yoloms/yoloms_syncbn_fast_8xb32-300e_coco.py'\n",
    "# Dataset configuration\n",
    "data_root = 'D:/UnB/IIA/YOLO-MS-IIA/coco_dataset'\n",
    "class_name = ('tree',)  # Your single class\n",
    "num_classes = 1\n",
    "metainfo = dict(classes=class_name, palette=[(0, 255, 0)])\n",
    "\n",
    "# Training parameters for fine-tuning\n",
    "max_epochs = 300  # Reduced from 300 for fine-tuning\n",
    "train_batch_size_per_gpu = 2  # Adjust based on your GPU memory\n",
    "val_batch_size_per_gpu = 1\n",
    "train_num_workers = 2\n",
    "val_num_workers = 1\n",
    "\n",
    "# Lower learning rate for fine-tuning\n",
    "base_lr = 0.0001\n",
    "\n",
    "\n",
    "# Model configuration - Update for single class\n",
    "model = dict(\n",
    "    bbox_head=dict(\n",
    "        head_module=dict(num_classes=num_classes)\n",
    "    ),\n",
    "    train_cfg=dict(\n",
    "        assigner=dict(num_classes=num_classes)\n",
    "    )\n",
    ")\n",
    "\n",
    "# Data configuration\n",
    "train_dataloader = dict(\n",
    "    batch_size=train_batch_size_per_gpu,\n",
    "    num_workers=train_num_workers,\n",
    "    dataset=dict(\n",
    "        data_root=data_root,\n",
    "        ann_file='annotations/instances_train.json',\n",
    "        data_prefix=dict(img='images/train/'),\n",
    "        metainfo=metainfo\n",
    "    )\n",
    ")\n",
    "\n",
    "val_dataloader = dict(\n",
    "    batch_size=val_batch_size_per_gpu,\n",
    "    num_workers=val_num_workers,\n",
    "    dataset=dict(\n",
    "        data_root=data_root,\n",
    "        ann_file='annotations/instances_val.json',\n",
    "        data_prefix=dict(img='images/val/'),\n",
    "        metainfo=metainfo,\n",
    "        test_mode=True\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add test dataloader\n",
    "test_dataloader = dict(\n",
    "    batch_size=val_batch_size_per_gpu,\n",
    "    num_workers=val_num_workers,\n",
    "    dataset=dict(\n",
    "        data_root=data_root,\n",
    "        ann_file='annotations/instances_test.json',\n",
    "        data_prefix=dict(img='images/test/'),\n",
    "        metainfo=metainfo,\n",
    "        test_mode=True\n",
    "    )\n",
    ")\n",
    "\n",
    "# Evaluators\n",
    "val_evaluator = dict(\n",
    "    type='mmdet.CocoMetric',\n",
    "    ann_file=f'{data_root}/annotations/instances_val.json',\n",
    "    metric='bbox',\n",
    "    format_only=False,\n",
    "    classwise=True,\n",
    ")\n",
    "\n",
    "test_evaluator = dict(\n",
    "    type='mmdet.CocoMetric',\n",
    "    ann_file=f'{data_root}/annotations/instances_test.json',\n",
    "    metric='bbox',\n",
    "    format_only=False,\n",
    "    classwise=True,\n",
    ")\n",
    "\n",
    "# Training config\n",
    "train_cfg = dict(\n",
    "    type='EpochBasedTrainLoop',\n",
    "    max_epochs=max_epochs,\n",
    "    val_interval=20\n",
    ")\n",
    "\n",
    "val_cfg = dict(type='ValLoop')\n",
    "test_cfg = dict(type='TestLoop')\n",
    "    \n",
    "# Auto-scaling learning rate\n",
    "auto_scale_lr = dict(enable=True, base_batch_size=16)\n",
    "\n",
    "# Hooks configuration\n",
    "default_hooks = dict(\n",
    "    checkpoint=dict(\n",
    "        type='CheckpointHook',\n",
    "        interval=20,\n",
    "        save_best='coco/bbox_mAP',\n",
    "        rule='greater',\n",
    "        max_keep_ckpts=5\n",
    "    ),\n",
    "    logger=dict(type='LoggerHook', interval=20)\n",
    ")\n",
    "\n",
    "# Load pretrained weights for fine-tuning\n",
    "load_from = 'D:/UnB/IIA/YOLO-MS-IIA/pre-trained.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c6d546",
   "metadata": {},
   "source": [
    "Esse código está salvo no arquivo `yoloms_trees_finetune.py` e pode ser utilizado para treinar o modelo YOLO-MS com os dados do dataset convertido para o formato COCO."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea32dcdb",
   "metadata": {},
   "source": [
    "## Fine-tune do modelo\n",
    "Como o dataset COCO e o arquivo de configuração do modelo, o próximo passo é realizar o treinamento de pesos do modelo YOLO-MS com os dados do dataset convertido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df77f235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python mmyolo/tools/train.py \"D:\\UnB\\IIA\\YOLO-MS-IIA\\yoloms_trees_finetune.py\" --work-dir \"D:\\UnB\\IIA\\YOLO-MS-IIA\\finetune\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9982381a",
   "metadata": {},
   "source": [
    "## Teste do modelo treinado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30923478",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
